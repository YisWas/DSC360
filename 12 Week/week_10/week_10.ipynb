{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 10 - Sentiment Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Text Preprocessing and Normalization - Starting on Page 570\n",
    "\n",
    "The author notes the file `Text Normalization Demo.ipynb`, which doesn't exist in his repo. However, I've created the same output here using the `Normalizer` created in Week 4. My code does not match the author's line-for-line but it has the same content."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several ways to get folders visible in Python. This way isn't the most elegant\n",
    "# but it works consistently. Replace my path with yours. The path you append to should be the\n",
    "# folder where your tokenizer Python class is located.\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\neugg\\OneDrive\\Documents\\GitHub\\dsc360-instructor\\12 Week\\week_4\\assignment')\n",
    "from text_normalizer import TextNormalizer\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "## Get the Data and Extract\n",
    "The ideas start on page 573, but I normalize the data unlike the author."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   review     50000 non-null  object\n 1   sentiment  50000 non-null  object\ndtypes: object(2)\nmemory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "orig_movie_data = pd.read_csv('data/movie_reviews.csv')\n",
    "orig_movie_data.info()"
   ]
  },
  {
   "source": [
    "## Run the Normalizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting TextNormalizer\n",
      "Done strip\n",
      "Done lower\n",
      "Done stopword\n",
      "Done char remove\n",
      "Done contract exp\n",
      "Done text lemm\n",
      "Done spec char remove\n"
     ]
    }
   ],
   "source": [
    "tn = TextNormalizer()\n",
    "movie_reviews_normalized = tn.normalize_corpus(corpus=orig_movie_data['review'])"
   ]
  },
  {
   "source": [
    "## Reassemble the clean data with the sentiments to create a clean DataFrame."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   review     50000 non-null  object\n 1   sentiment  50000 non-null  object\ndtypes: object(2)\nmemory usage: 781.4+ KB\n                                              review sentiment\n0  one reviewers mentioned watching oz episode ho...  positive\n1  wonderful little production filming technique ...  positive\n2  thought wonderful way spend time hot summer we...  positive\n3  basically family little boy jake thinks zombie...  negative\n4  petter mattei love time money visually stunnin...  positive\n"
     ]
    }
   ],
   "source": [
    "movie_reviews_clean_df = pd.DataFrame({'review': movie_reviews_normalized, 'sentiment': orig_movie_data['sentiment']})\n",
    "movie_reviews_clean_df.info()\n",
    "print(movie_reviews_clean_df.head())\n",
    "# Save the clean data\n",
    "movie_reviews_clean_df.to_csv('data/movie_reviews_clean.csv', index=False)"
   ]
  },
  {
   "source": [
    "## Unsupervised Lexicon-Based Models - Starting on page 573\n",
    "You can start here if you want to skip cleaning the data.\n",
    "\n",
    "**NOTE** the model_evaluation_utils.py file is from the author, is referenced in the book, but is actually contained in the GitHub for a totally different book (which may be the new book or something): https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/model_evaluation_utils.py. But I also had to modify that code, so the `model_evaluation_utils.py` contained in this GitHub is the one that works."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                              review sentiment\n0  one reviewers mentioned watching oz episode ho...  positive\n1  wonderful little production filming technique ...  positive\n2  thought wonderful way spend time hot summer we...  positive\n3  basically family little boy jake thinks zombie...  negative\n4  petter mattei love time money visually stunnin...  positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import model_evaluation_utils as meu\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "dataset = pd.read_csv(r'data/movie_reviews_clean.csv')\n",
    "\n",
    "print(dataset.head())\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# extract data for model evaluation\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "sample_review_ids = [7626, 3533, 13010]"
   ]
  },
  {
   "source": [
    "## Text Blob - Starting on page 576"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "REVIEW: comment stupid movie acting average worse screenplay sense skip \nActual Sentiment: negative\nPredicted Sentiment polarity: -0.3375\n------------------------------------------------------------\nREVIEW:  care people voted movie bad want truth good movie every thing movie have really get one \nActual Sentiment: positive\nPredicted Sentiment polarity: 0.06666666666666671\n------------------------------------------------------------\nREVIEW: worst horror film ever funniest film ever rolled one got see film cheap unbeliaveble see really p s watch carrot\nActual Sentiment: positive\nPredicted Sentiment polarity: -0.13333333333333333\n------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import textblob\n",
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    print('Predicted Sentiment polarity:', textblob.TextBlob(review).sentiment.polarity)\n",
    "    print('-'*60)"
   ]
  },
  {
   "source": [
    "### Checking sentiments - page 577"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7641\n",
      "Precision: 0.7641\n",
      "Recall: 0.7641\n",
      "F1 Score: 0.7641\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.76      0.76      7510\n",
      "    negative       0.76      0.76      0.76      7490\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     15000\n",
      "   macro avg       0.76      0.76      0.76     15000\n",
      "weighted avg       0.76      0.76      0.76     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       5734     1776\n",
      "        negative       1762     5728\n"
     ]
    }
   ],
   "source": [
    "sentiment_polarity = [textblob.TextBlob(review).sentiment.polarity for review in test_reviews]\n",
    "predicted_sentiments = ['positive' if score >= 0.1 else 'negative' for score in sentiment_polarity]\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, classes=['positive', 'negative'])"
   ]
  },
  {
   "source": [
    "## AFINN Lexicon - Page 578"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "REVIEW: comment stupid movie acting average worse screenplay sense skip \nActual Sentiment: negative\nPredicted Sentiment polarity: -5.0\n------------------------------------------------------------\nREVIEW:  care people voted movie bad want truth good movie every thing movie have really get one \nActual Sentiment: positive\nPredicted Sentiment polarity: 3.0\n------------------------------------------------------------\nREVIEW: worst horror film ever funniest film ever rolled one got see film cheap unbeliaveble see really p s watch carrot\nActual Sentiment: positive\nPredicted Sentiment polarity: -3.0\n------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "afn = Afinn(emoticons=True) \n",
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    print('Predicted Sentiment polarity:', afn.score(review))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7008\n",
      "Precision: 0.7211\n",
      "Recall: 0.7008\n",
      "F1 Score: 0.6937\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.65      0.85      0.74      7510\n",
      "    negative       0.79      0.55      0.65      7490\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     15000\n",
      "   macro avg       0.72      0.70      0.69     15000\n",
      "weighted avg       0.72      0.70      0.69     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6405     1105\n",
      "        negative       3383     4107\n"
     ]
    }
   ],
   "source": [
    "sentiment_polarity = [afn.score(review) for review in test_reviews]\n",
    "predicted_sentiments = ['positive' if score >= 0.1 else 'negative' for score in sentiment_polarity]\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, classes=['positive', 'negative'])"
   ]
  },
  {
   "source": [
    "## SentiWordNet Lexicon - Starting on page 580\n",
    "My code differs to fix bugs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Positive Polarity Score: 0.875\nNegative Polarity Score: 0.125\nObjective Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "awesome = list(swn.senti_synsets('awesome', 'a'))[0]\n",
    "print('Positive Polarity Score:', awesome.pos_score())\n",
    "print('Negative Polarity Score:', awesome.neg_score())\n",
    "print('Objective Score:', awesome.obj_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "REVIEW: comment stupid movie acting average worse screenplay sense skip \nActual Sentiment: negative\ncomment NN\nstupid JJ\nmovie NN\nacting VBG\naverage JJ\nworse JJ\nscreenplay NN\nsense NN\nskip NN\n     SENTIMENT STATS:                                      \n  Predicted Sentiment Objectivity Positive Negative Overall\n0            negative        0.66     0.06     0.28   -0.22\n------------------------------------------------------------\nREVIEW:  care people voted movie bad want truth good movie every thing movie have really get one \nActual Sentiment: positive\ncare NN\npeople NNS\nvoted VBD\nmovie NN\nbad JJ\nwant VBP\ntruth NN\ngood JJ\nmovie NN\nevery DT\nthing NN\nmovie NN\nhave VBP\nreally RB\nget VB\none CD\n     SENTIMENT STATS:                                      \n  Predicted Sentiment Objectivity Positive Negative Overall\n0            positive        0.74     0.15     0.11    0.04\n------------------------------------------------------------\nREVIEW: worst horror film ever funniest film ever rolled one got see film cheap unbeliaveble see really p s watch carrot\nActual Sentiment: positive\nworst JJS\nhorror NN\nfilm NN\never RB\nfunniest JJS\nfilm NN\never RB\nrolled VBD\none CD\ngot VBD\nsee NN\nfilm NN\ncheap JJ\nunbeliaveble JJ\nsee NN\nreally RB\np JJ\ns NN\nwatch NN\ncarrot NN\n     SENTIMENT STATS:                                      \n  Predicted Sentiment Objectivity Positive Negative Overall\n0            negative        0.83     0.04     0.13   -0.09\n------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def analyze_sentiment_sentiwordnet_lexicon(review, verbose):\n",
    "    # tokenize and POS tag text tokens\n",
    "    text_tokens = nltk.word_tokenize(review)\n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "    # get wordnet synsets based on POS tags\n",
    "    # get sentiment scores if synsets are found\n",
    "    for word, tag in tagged_text:\n",
    "        if verbose:\n",
    "            print(word, tag)\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and swn.senti_synsets(word, 'n'):\n",
    "            ss_set = swn.senti_synsets(word, 'n')\n",
    "        elif 'VB' in tag and swn.senti_synsets(word, 'v'):\n",
    "            ss_set = swn.senti_synsets(word, 'v')\n",
    "        elif 'JJ' in tag and swn.senti_synsets(word, 'a'):\n",
    "            ss_set = swn.senti_synsets(word, 'a')\n",
    "        elif 'RB' in tag and swn.senti_synsets(word, 'r'):\n",
    "            ss_set = swn.senti_synsets(word, 'r')\n",
    "        # if senti-synset is found        \n",
    "        if ss_set:\n",
    "            for synst in ss_set:\n",
    "                # add scores for all found synsets\n",
    "                pos_score += synst.pos_score()\n",
    "                neg_score += synst.neg_score()\n",
    "                obj_score += synst.obj_score()\n",
    "                token_count += 1\n",
    "    \n",
    "    # aggregate final scores\n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        # to display results in a nice table\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score,\n",
    "                                         norm_pos_score, norm_neg_score,\n",
    "                                         norm_final_score]],\n",
    "                                         columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment', 'Objectivity',\n",
    "                                                                       'Positive', 'Negative', 'Overall']], \n",
    "                                                              codes=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "        \n",
    "    return final_sentiment\n",
    "\n",
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    pred = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True)\n",
    "    print('-'*60)"
   ]
  },
  {
   "source": [
    "## Predict Sentiment on Test Reviews and Evaluation Performance - Page 583"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.5847\n",
      "Precision: 0.6905\n",
      "Recall: 0.5847\n",
      "F1 Score: 0.5172\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.55      0.96      0.70      7510\n",
      "    negative       0.83      0.21      0.34      7490\n",
      "\n",
      "   micro avg       0.58      0.58      0.58     15000\n",
      "   macro avg       0.69      0.58      0.52     15000\n",
      "weighted avg       0.69      0.58      0.52     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       7193      317\n",
      "        negative       5913     1577\n"
     ]
    }
   ],
   "source": [
    "# note that the corpus is already cleaned to the first line is skipped (just use test_reviews)\n",
    "predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in test_reviews]\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, \n",
    "    predicted_labels=predicted_sentiments, classes=['positive', 'negative'])"
   ]
  },
  {
   "source": [
    "## Vader Lexicon - Starting on Page 585\n",
    "Note that my code does not clean the text because it's already cleaned."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\neugg\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# only need to run the following line of code once\n",
    "nltk.download('vader_lexicon') \n",
    "def analyze_sentiment_vader_lexicon(review, threshold=0.1, verbose=False):\n",
    "    # analyze the sentiment for review\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    # get aggregate scores and final sentiment\n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 'positive' if agg_score >= threshold\\\n",
    "                                   else 'negative'\n",
    "    if verbose:\n",
    "        # display detailed sentiment statistics\n",
    "        positive = str(round(scores['pos'], 2)*100)+'%'\n",
    "        final = round(agg_score, 2)\n",
    "        negative = str(round(scores['neg'], 2)*100)+'%'\n",
    "        neutral = str(round(scores['neu'], 2)*100)+'%'\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive,\n",
    "                                        negative, neutral]],\n",
    "                                        columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment', 'Polarity Score',\n",
    "                                                                       'Positive', 'Negative',\n",
    "                                                                       'Neutral']], \n",
    "                                                              codes=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "    \n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "REVIEW: comment stupid movie acting average worse screenplay sense skip \nActual Sentiment: negative\n     SENTIMENT STATS:                                         \n  Predicted Sentiment Polarity Score Positive Negative Neutral\n0            negative          -0.76     0.0%    48.0%   52.0%\n------------------------------------------------------------\nREVIEW:  care people voted movie bad want truth good movie every thing movie have really get one \nActual Sentiment: positive\n     SENTIMENT STATS:                                                     \n  Predicted Sentiment Polarity Score Positive             Negative Neutral\n0            positive           0.64    40.0%  14.000000000000002%   46.0%\n------------------------------------------------------------\nREVIEW: worst horror film ever funniest film ever rolled one got see film cheap unbeliaveble see really p s watch carrot\nActual Sentiment: positive\n     SENTIMENT STATS:                                      \\\n  Predicted Sentiment Polarity Score             Positive   \n0            negative          -0.64  14.000000000000002%   \n\n                                            \n              Negative             Neutral  \n0  28.999999999999996%  56.99999999999999%  \n------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    pred = analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=True)\n",
    "    print('-'*60)"
   ]
  },
  {
   "source": [
    "### Page 587"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.6895\n",
      "Precision: 0.7081\n",
      "Recall: 0.6895\n",
      "F1 Score: 0.6823\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.65      0.84      0.73      7510\n",
      "    negative       0.77      0.54      0.63      7490\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     15000\n",
      "   macro avg       0.71      0.69      0.68     15000\n",
      "weighted avg       0.71      0.69      0.68     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6306     1204\n",
      "        negative       3453     4037\n"
     ]
    }
   ],
   "source": [
    "predicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=False) for review in test_reviews]\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, \n",
    "    predicted_labels=predicted_sentiments, classes=['positive', 'negative'])"
   ]
  },
  {
   "source": [
    "## Classifying Sentiment with Supervised Learning - Starting on page 589\n",
    "Text is already cleaned and is reloaded in case you want to start here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "import model_evaluation_utils as meu\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                              review sentiment\n0  one reviewers mentioned watching oz episode ho...  positive\n1  wonderful little production filming technique ...  positive\n2  thought wonderful way spend time hot summer we...  positive\n3  basically family little boy jake thinks zombie...  negative\n4  petter mattei love time money visually stunnin...  positive\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'data/movie_reviews_clean.csv')\n",
    "print(dataset.head())\n",
    "\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# build train and test datasets\n",
    "# NOTE: this is NOT how to split test and train, but it follows the book\n",
    "train_reviews = reviews[:35000]\n",
    "train_sentiments = sentiments[:35000]\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "\n",
    "# skipping normalizing the dataset, already normalized"
   ]
  },
  {
   "source": [
    "## Traditional Supervised Machine Learning Models - Starting on page 590"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BOW model: TRAIN features shape: (35000, 2487514)\nTEST features shape: (15000, 2487514) \n\nTFIDF mode: TRAIN features shape: (35000, 2487514)\nTEST features shape: (15000, 2487514)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(train_reviews)\n",
    "\n",
    "# build TFIDF features on train reveiws\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2), sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(train_reviews)\n",
    "\n",
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(test_reviews)\n",
    "tv_test_features= tv.transform(test_reviews)\n",
    "\n",
    "print('BOW model: TRAIN features shape:', cv_train_features.shape)\n",
    "print('TEST features shape:', cv_test_features.shape, '\\n')\n",
    "\n",
    "print('TFIDF mode: TRAIN features shape:', tv_train_features.shape)\n",
    "print('TEST features shape:', tv_test_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.9017\n",
      "Precision: 0.9017\n",
      "Recall: 0.9017\n",
      "F1 Score: 0.9017\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.90      0.91      0.90      7510\n",
      "    negative       0.90      0.90      0.90      7490\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6797      713\n",
      "        negative        762     6728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1)\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)\n",
    "\n",
    "# Logistic Regression model on BOW features\n",
    "lr_bow_predictions = meu.train_predict_model(classifier=lr, train_features=cv_train_features, train_labels=train_sentiments,\n",
    "                                             test_features=cv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions, classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8945\n",
      "Precision: 0.8948\n",
      "Recall: 0.8945\n",
      "F1 Score: 0.8945\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.88      0.91      0.90      7510\n",
      "    negative       0.90      0.88      0.89      7490\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6815      695\n",
      "        negative        887     6603\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model on TF-IDF features (page 592)\n",
    "lr_tfidf_predictions = meu.train_predict_model(classifier=lr, train_features=tv_train_features, train_labels=train_sentiments,\n",
    "                                               test_features=tv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions, classes=['positive', 'negative'])"
   ]
  },
  {
   "source": [
    "## Newer Supervised Deep Learning Models - Starting on page 593\n",
    "\n",
    "I did not go into the TensorFlow / Keras models. You'll see some of this is DSC 410 and at this point running these models for \n",
    "text analytics can require very intensive CPU power."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Analyzing Sentiment Causation and Interpreting Predictive Models\n",
    "Starting on page 615.\n",
    "\n",
    "**NOTE** the code at this point doesn't appear to exist in the author's repository and the code itself is repeated in an entirely different book (*Practical Machine Learning with Python: A Problem-Solver's Guide to Building Real-World Intelligent Systems*) from the same author.\n",
    "\n",
    "My code is slightly different because I did not clean the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(train_reviews)\n",
    "# build Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(cv_train_features, train_sentiments)\n",
    "\n",
    "# Build Text Classification Pipeline\n",
    "lr_pipeline = make_pipeline(cv, lr)\n",
    "\n",
    "# save the list of prediction classes (positive, negative)\n",
    "classes = list(lr_pipeline.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   negative  positive\n",
       "0  0.188661  0.811339\n",
       "1  0.813330  0.186670"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>negative</th>\n      <th>positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.188661</td>\n      <td>0.811339</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.813330</td>\n      <td>0.186670</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# skipped normalizing the new reviews - these's no point since it's so small and can be normalized manually\n",
    "new_corpus = ['the lord of the rings is an excellent movie',\n",
    "              'i hated the recent movie on tv, it was so bad']\n",
    "lr_pipeline.predict(new_corpus)\n",
    "\n",
    "#array(['positive', 'negative'], dtype=object) - THIS DOES NOTHING\n",
    "\n",
    "pd.DataFrame(lr_pipeline.predict_proba(new_corpus), columns=classes)"
   ]
  },
  {
   "source": [
    "### You need to install the `skater` package - `pip install skater`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skater'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0c5ddca05b75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# page 617\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mskater\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_interpretation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlime_text\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# helper function for model interpretation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skater'"
     ]
    }
   ],
   "source": [
    "# page 617\n",
    "from skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=classes)\n",
    "# helper function for model interpretation\n",
    "def interpret_classification_model_prediction(doc_index, norm_corpus, corpus,\n",
    "                                              prediction_labels, explainer_obj):\n",
    "    # display model prediction and actual sentiments\n",
    "    print(\"Test document index: {index}\\nActual sentiment: {actual} \\\n",
    "                                       \\nPredicted sentiment: {predicted}\"\n",
    "      .format(index=doc_index, actual=prediction_labels[doc_index],\n",
    "              predicted=lr_pipeline.predict([norm_corpus[doc_index]])))\n",
    "    # display actual review content    print(\"\\nReview:\", corpus[doc_index])\n",
    "    # display prediction probabilities    print(\"\\nModel Prediction Probabilities:\")\n",
    "    for probs in zip(classes, lr_pipeline.predict_proba([norm_corpus[doc_index]])[0]):\n",
    "        print(probs)\n",
    "    # display model prediction interpretation\n",
    "    exp = explainer.explain_instance(norm_corpus[doc_index],\n",
    "                                     lr_pipeline.predict_proba, num_features=10,\n",
    "                                     labels=[1])\n",
    "    exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}