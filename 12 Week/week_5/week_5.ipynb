{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 5 - Text Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Setup and Text Cleanup\n",
    "Code starts on page 203."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                            Document Category\n0                     The sky is blue and beautiful.  weather\n1                  Love this blue and beautiful sky!  weather\n2       The quick brown fox jumps over the lazy dog.  animals\n3  A king's breakfast has sausages, ham, bacon, e...     food\n4      I love green eggs, ham, saussages, and bacon!     food\n5   The brown fox is quick and the blue dog is lazy!  animals\n6  The sky is very blue and the sky is very beaut...  weather\n7        The dog is lazy but the brown fox is quick!  animals \n\n['sky blue beautiful' 'love blue beautiful sky'\n 'quick brown fox jumps lazy dog'\n 'kings breakfast sausages ham bacon eggs toast beans'\n 'love green eggs ham saussages bacon' 'brown fox quick blue dog lazy'\n 'sky blue sky beautiful today' 'dog lazy brown fox quick'] \n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', None, 'display.max_columns', None)\n",
    "\n",
    "# building a corpus of documents\n",
    "corpus = [\n",
    "    'The sky is blue and beautiful.',\n",
    "    'Love this blue and beautiful sky!',\n",
    "    'The quick brown fox jumps over the lazy dog.',\n",
    "    'A king\\'s breakfast has sausages, ham, bacon, eggs, toast, and beans.',\n",
    "    'I love green eggs, ham, saussages, and bacon!',\n",
    "    'The brown fox is quick and the blue dog is lazy!',\n",
    "    'The sky is very blue and the sky is very beautiful today.',\n",
    "    'The dog is lazy but the brown fox is quick!'\n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals',\n",
    "          'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "print(corpus_df, '\\n')\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('English')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lowercase and remove special characters\\whitespace\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    #tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "print(norm_corpus, '\\n')"
   ]
  },
  {
   "source": [
    "## Bag of Words Model\n",
    "Starting on page 208"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bag of Words Model\n  (0, 18)\t1\n  (0, 3)\t1\n  (0, 2)\t1\n  (1, 18)\t1\n  (1, 3)\t1\n  (1, 2)\t1\n  (1, 14)\t1\n  (2, 15)\t1\n  (2, 5)\t1\n  (2, 8)\t1\n  (2, 11)\t1\n  (2, 13)\t1\n  (2, 6)\t1\n  (3, 12)\t1\n  (3, 4)\t1\n  (3, 16)\t1\n  (3, 10)\t1\n  (3, 0)\t1\n  (3, 7)\t1\n  (3, 19)\t1\n  (3, 1)\t1\n  (4, 14)\t1\n  (4, 10)\t1\n  (4, 0)\t1\n  (4, 7)\t1\n  (4, 9)\t1\n  (4, 17)\t1\n  (5, 3)\t1\n  (5, 15)\t1\n  (5, 5)\t1\n  (5, 8)\t1\n  (5, 13)\t1\n  (5, 6)\t1\n  (6, 18)\t2\n  (6, 3)\t1\n  (6, 2)\t1\n  (6, 20)\t1\n  (7, 15)\t1\n  (7, 5)\t1\n  (7, 8)\t1\n  (7, 13)\t1\n  (7, 6)\t1 \n\n[[0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n [0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n [0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0]\n [1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0]\n [1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0]\n [0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0]\n [0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n [0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0]] \n\n   bacon  beans  beautiful  blue  breakfast  brown  dog  eggs  fox  green  \\\n0      0      0          1     1          0      0    0     0    0      0   \n1      0      0          1     1          0      0    0     0    0      0   \n2      0      0          0     0          0      1    1     0    1      0   \n3      1      1          0     0          1      0    0     1    0      0   \n4      1      0          0     0          0      0    0     1    0      1   \n5      0      0          0     1          0      1    1     0    1      0   \n6      0      0          1     1          0      0    0     0    0      0   \n7      0      0          0     0          0      1    1     0    1      0   \n\n   ham  jumps  kings  lazy  love  quick  sausages  saussages  sky  toast  \\\n0    0      0      0     0     0      0         0          0    1      0   \n1    0      0      0     0     1      0         0          0    1      0   \n2    0      1      0     1     0      1         0          0    0      0   \n3    1      0      1     0     0      0         1          0    0      1   \n4    1      0      0     0     1      0         0          1    0      0   \n5    0      0      0     1     0      1         0          0    0      0   \n6    0      0      0     0     0      0         0          0    2      0   \n7    0      0      0     1     0      1         0          0    0      0   \n\n   today  \n0      0  \n1      0  \n2      0  \n3      0  \n4      0  \n5      0  \n6      1  \n7      0   \n\n   bacon eggs  beautiful sky  beautiful today  blue beautiful  blue dog  \\\n0           0              0                0               1         0   \n1           0              1                0               1         0   \n2           0              0                0               0         0   \n3           1              0                0               0         0   \n4           0              0                0               0         0   \n5           0              0                0               0         1   \n6           0              0                1               0         0   \n7           0              0                0               0         0   \n\n   blue sky  breakfast sausages  brown fox  dog lazy  eggs ham  eggs toast  \\\n0         0                   0          0         0         0           0   \n1         0                   0          0         0         0           0   \n2         0                   0          1         0         0           0   \n3         0                   1          0         0         0           1   \n4         0                   0          0         0         1           0   \n5         0                   0          1         1         0           0   \n6         1                   0          0         0         0           0   \n7         0                   0          1         1         0           0   \n\n   fox jumps  fox quick  green eggs  ham bacon  ham saussages  jumps lazy  \\\n0          0          0           0          0              0           0   \n1          0          0           0          0              0           0   \n2          1          0           0          0              0           1   \n3          0          0           0          1              0           0   \n4          0          0           1          0              1           0   \n5          0          1           0          0              0           0   \n6          0          0           0          0              0           0   \n7          0          1           0          0              0           0   \n\n   kings breakfast  lazy brown  lazy dog  love blue  love green  quick blue  \\\n0                0           0         0          0           0           0   \n1                0           0         0          1           0           0   \n2                0           0         1          0           0           0   \n3                1           0         0          0           0           0   \n4                0           0         0          0           1           0   \n5                0           0         0          0           0           1   \n6                0           0         0          0           0           0   \n7                0           1         0          0           0           0   \n\n   quick brown  sausages ham  saussages bacon  sky beautiful  sky blue  \\\n0            0             0                0              0         1   \n1            0             0                0              0         0   \n2            1             0                0              0         0   \n3            0             1                0              0         0   \n4            0             0                1              0         0   \n5            0             0                0              0         0   \n6            0             0                0              1         1   \n7            0             0                0              0         0   \n\n   toast beans  \n0            0  \n1            0  \n2            0  \n3            1  \n4            0  \n5            0  \n6            0  \n7            0   \n\n"
     ]
    }
   ],
   "source": [
    "print('Bag of Words Model')\n",
    "# starting on page 208\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# get bag of words features in sparse format\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "# view non-zero feature positions in the sparse matrix\n",
    "print(cv_matrix, '\\n')\n",
    "\n",
    "# view dense representation\n",
    "# warning - might give a memory error if the data is too big\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "print(cv_matrix, '\\n')\n",
    "\n",
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "#show document feature vectors\n",
    "cv_df = pd.DataFrame(cv_matrix, columns=vocab)\n",
    "print(cv_df, '\\n')\n",
    "\n",
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "bv = CountVectorizer(ngram_range=(2,2))\n",
    "bv_matrix = bv.fit_transform(norm_corpus)\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab = bv.get_feature_names()\n",
    "bv_df = pd.DataFrame(bv_matrix, columns=vocab)\n",
    "print(bv_df, '\\n')"
   ]
  },
  {
   "source": [
    "## Tf-Idf Transformer - Starting on Page 213"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tfidf transformer:\n   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n3   0.31   0.38       0.00  0.00       0.38   0.00  0.00  0.31  0.00   0.00   \n4   0.38   0.00       0.00  0.00       0.00   0.00  0.00  0.38  0.00   0.46   \n5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n6   0.00   0.00       0.36  0.32       0.00   0.00  0.00  0.00  0.00   0.00   \n7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n\n    ham  jumps  kings  lazy  love  quick  sausages  saussages   sky  toast  \\\n0  0.00   0.00   0.00  0.00  0.00   0.00      0.00       0.00  0.60   0.00   \n1  0.00   0.00   0.00  0.00  0.57   0.00      0.00       0.00  0.49   0.00   \n2  0.00   0.53   0.00  0.38  0.00   0.38      0.00       0.00  0.00   0.00   \n3  0.31   0.00   0.38  0.00  0.00   0.00      0.38       0.00  0.00   0.38   \n4  0.38   0.00   0.00  0.00  0.38   0.00      0.00       0.46  0.00   0.00   \n5  0.00   0.00   0.00  0.42  0.00   0.42      0.00       0.00  0.00   0.00   \n6  0.00   0.00   0.00  0.00  0.00   0.00      0.00       0.00  0.72   0.00   \n7  0.00   0.00   0.00  0.45  0.00   0.45      0.00       0.00  0.00   0.00   \n\n   today  \n0    0.0  \n1    0.0  \n2    0.0  \n3    0.0  \n4    0.0  \n5    0.0  \n6    0.5  \n7    0.0   \n\n"
     ]
    }
   ],
   "source": [
    "print('tfidf transformer:')\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tt = TfidfTransformer(norm = 'l2', use_idf=True)\n",
    "tt_matrix = tt.fit_transform(cv_matrix)\n",
    "tt_matrix = tt_matrix.toarray()\n",
    "vocab = cv.get_feature_names()\n",
    "print(pd.DataFrame(np.round(tt_matrix, 2), columns=vocab), '\\n')"
   ]
  },
  {
   "source": [
    "## tfidfvectorizer, page 214"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tfidf vectorizer:\n   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n3   0.31   0.38       0.00  0.00       0.38   0.00  0.00  0.31  0.00   0.00   \n4   0.38   0.00       0.00  0.00       0.00   0.00  0.00  0.38  0.00   0.46   \n5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n6   0.00   0.00       0.36  0.32       0.00   0.00  0.00  0.00  0.00   0.00   \n7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n\n    ham  jumps  kings  lazy  love  quick  sausages  saussages   sky  toast  \\\n0  0.00   0.00   0.00  0.00  0.00   0.00      0.00       0.00  0.60   0.00   \n1  0.00   0.00   0.00  0.00  0.57   0.00      0.00       0.00  0.49   0.00   \n2  0.00   0.53   0.00  0.38  0.00   0.38      0.00       0.00  0.00   0.00   \n3  0.31   0.00   0.38  0.00  0.00   0.00      0.38       0.00  0.00   0.38   \n4  0.38   0.00   0.00  0.00  0.38   0.00      0.00       0.46  0.00   0.00   \n5  0.00   0.00   0.00  0.42  0.00   0.42      0.00       0.00  0.00   0.00   \n6  0.00   0.00   0.00  0.00  0.00   0.00      0.00       0.00  0.72   0.00   \n7  0.00   0.00   0.00  0.45  0.00   0.45      0.00       0.00  0.00   0.00   \n\n   today  \n0    0.0  \n1    0.0  \n2    0.0  \n3    0.0  \n4    0.0  \n5    0.0  \n6    0.5  \n7    0.0   \n\n"
     ]
    }
   ],
   "source": [
    "print('tfidf vectorizer:')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2', use_idf=True, smooth_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "# this part is not in the book - save the tv_matrix for use later on\n",
    "import os\n",
    "np.save('tv_matrix.npy', tv_matrix)\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "print(pd.DataFrame(np.round(tv_matrix, 2), columns=vocab), '\\n')"
   ]
  },
  {
   "source": [
    "## Understanding the TF-DF Model - starting on page 215"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature Names: ['blue', 'sky', 'beautiful', 'love', 'eggs', 'beans', 'jumps', 'lazy', 'quick', 'saussages', 'ham', 'today', 'green', 'bacon', 'kings', 'breakfast', 'toast', 'sausages', 'dog', 'fox', 'brown']\nDefault Feature Dict: {'blue': 0, 'sky': 0, 'beautiful': 0, 'love': 0, 'eggs': 0, 'beans': 0, 'jumps': 0, 'lazy': 0, 'quick': 0, 'saussages': 0, 'ham': 0, 'today': 0, 'green': 0, 'bacon': 0, 'kings': 0, 'breakfast': 0, 'toast': 0, 'sausages': 0, 'dog': 0, 'fox': 0, 'brown': 0} \n\nBOW features:\n    sky  blue  beautiful  love  eggs  beans  jumps  lazy  quick  saussages  \\\n0    1     1          1     0     0      0      0     0      0          0   \n1    1     1          1     1     0      0      0     0      0          0   \n2    0     0          0     0     0      0      1     1      1          0   \n3    0     0          0     0     1      1      0     0      0          0   \n4    0     0          0     1     1      0      0     0      0          1   \n5    0     1          0     0     0      0      0     1      1          0   \n6    2     1          1     0     0      0      0     0      0          0   \n7    0     0          0     0     0      0      0     1      1          0   \n\n   ham  today  green  bacon  kings  breakfast  toast  sausages  dog  fox  \\\n0    0      0      0      0      0          0      0         0    0    0   \n1    0      0      0      0      0          0      0         0    0    0   \n2    0      0      0      0      0          0      0         0    1    1   \n3    1      0      0      1      1          1      1         1    0    0   \n4    1      0      1      1      0          0      0         0    0    0   \n5    0      0      0      0      0          0      0         0    1    1   \n6    0      1      0      0      0          0      0         0    0    0   \n7    0      0      0      0      0          0      0         0    1    1   \n\n   brown  \n0      0  \n1      0  \n2      1  \n3      0  \n4      0  \n5      1  \n6      0  \n7      1  \n"
     ]
    }
   ],
   "source": [
    "# get unique words as feature names\n",
    "# different output than book\n",
    "unique_words = list(set([word for doc in [doc.split() for doc in norm_corpus] for word in doc]))\n",
    "def_feature_dict = {w: 0 for w in unique_words}\n",
    "print('Feature Names:', unique_words)\n",
    "print('Default Feature Dict:', def_feature_dict, '\\n')\n",
    "\n",
    "# page 216\n",
    "from collections import Counter\n",
    "# build bag of words features for each document - term frequencies\n",
    "bow_features = []\n",
    "for doc in norm_corpus:\n",
    "    bow_feature_doc = Counter(doc.split())\n",
    "    all_features = Counter(def_feature_dict)\n",
    "    bow_feature_doc.update(all_features)\n",
    "    bow_features.append(bow_feature_doc)\n",
    "\n",
    "bow_features = pd.DataFrame(bow_features)\n",
    "print('BOW features:\\n', bow_features)"
   ]
  },
  {
   "source": [
    "## Document Frequencies - starting on page 216"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Smooth DF:\n    sky  blue  beautiful  love  eggs  beans  jumps  lazy  quick  saussages  \\\n0    4     5          4     3     3      2      2     4      4          2   \n\n   ham  today  green  bacon  kings  breakfast  toast  sausages  dog  fox  \\\n0    3      2      2      3      2          2      2         2    4    4   \n\n   brown  \n0      4   \n\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp \n",
    "feature_names = list(bow_features.columns)\n",
    "\n",
    "# build the document frequency matrix\n",
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "df = 1 + df # adding 1 to smoothen idf later\n",
    "\n",
    "# show smoothened document frequencies\n",
    "print('Smooth DF:\\n', pd.DataFrame([df], columns=feature_names), '\\n')"
   ]
  },
  {
   "source": [
    "## IDF - page 217"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Smooth IDFs:\n     sky  blue  beautiful  love  eggs  beans  jumps  lazy  quick  saussages  \\\n0  1.81  1.59       1.81   2.1   2.1    2.5    2.5  1.81   1.81        2.5   \n\n   ham  today  green  bacon  kings  breakfast  toast  sausages   dog   fox  \\\n0  2.1    2.5    2.5    2.1    2.5        2.5    2.5       2.5  1.81  1.81   \n\n   brown  \n0   1.81   \n\n"
     ]
    }
   ],
   "source": [
    "# compute inverse document frequencies\n",
    "total_docs = 1 + len(norm_corpus)\n",
    "idf = 1.0 + np.log(float(total_docs) / df) \n",
    "\n",
    "# show smoothened IDFs\n",
    "print('Smooth IDFs:\\n', pd.DataFrame([np.round(idf, 2)], columns=feature_names), '\\n')"
   ]
  },
  {
   "source": [
    "## Tdf-Idf"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Diagonal matrix:\n       0     1     2    3    4    5    6     7     8    9    10   11   12   13  \\\n0   1.81  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n1   0.00  1.59  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n2   0.00  0.00  1.81  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n3   0.00  0.00  0.00  2.1  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n4   0.00  0.00  0.00  0.0  2.1  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n5   0.00  0.00  0.00  0.0  0.0  2.5  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n6   0.00  0.00  0.00  0.0  0.0  0.0  2.5  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n7   0.00  0.00  0.00  0.0  0.0  0.0  0.0  1.81  0.00  0.0  0.0  0.0  0.0  0.0   \n8   0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  1.81  0.0  0.0  0.0  0.0  0.0   \n9   0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  2.5  0.0  0.0  0.0  0.0   \n10  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  2.1  0.0  0.0  0.0   \n11  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  2.5  0.0  0.0   \n12  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  2.5  0.0   \n13  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  2.1   \n14  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n15  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n16  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n17  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n18  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n19  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n20  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0   \n\n     14   15   16   17    18    19    20  \n0   0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n1   0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n2   0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n3   0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n4   0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n5   0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n6   0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n7   0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n8   0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n9   0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n10  0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n11  0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n12  0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n13  0.0  0.0  0.0  0.0  0.00  0.00  0.00  \n14  2.5  0.0  0.0  0.0  0.00  0.00  0.00  \n15  0.0  2.5  0.0  0.0  0.00  0.00  0.00  \n16  0.0  0.0  2.5  0.0  0.00  0.00  0.00  \n17  0.0  0.0  0.0  2.5  0.00  0.00  0.00  \n18  0.0  0.0  0.0  0.0  1.81  0.00  0.00  \n19  0.0  0.0  0.0  0.0  0.00  1.81  0.00  \n20  0.0  0.0  0.0  0.0  0.00  0.00  1.81   \n\nRaw TF-IDF feature matrix\n     sky  blue  beautiful  love  eggs  beans  jumps  lazy  quick  saussages  \\\n0  1.81  1.59       1.81   0.0   0.0    0.0    0.0  0.00   0.00        0.0   \n1  1.81  1.59       1.81   2.1   0.0    0.0    0.0  0.00   0.00        0.0   \n2  0.00  0.00       0.00   0.0   0.0    0.0    2.5  1.81   1.81        0.0   \n3  0.00  0.00       0.00   0.0   2.1    2.5    0.0  0.00   0.00        0.0   \n4  0.00  0.00       0.00   2.1   2.1    0.0    0.0  0.00   0.00        2.5   \n5  0.00  1.59       0.00   0.0   0.0    0.0    0.0  1.81   1.81        0.0   \n6  3.62  1.59       1.81   0.0   0.0    0.0    0.0  0.00   0.00        0.0   \n7  0.00  0.00       0.00   0.0   0.0    0.0    0.0  1.81   1.81        0.0   \n\n   ham  today  green  bacon  kings  breakfast  toast  sausages   dog   fox  \\\n0  0.0    0.0    0.0    0.0    0.0        0.0    0.0       0.0  0.00  0.00   \n1  0.0    0.0    0.0    0.0    0.0        0.0    0.0       0.0  0.00  0.00   \n2  0.0    0.0    0.0    0.0    0.0        0.0    0.0       0.0  1.81  1.81   \n3  2.1    0.0    0.0    2.1    2.5        2.5    2.5       2.5  0.00  0.00   \n4  2.1    0.0    2.5    2.1    0.0        0.0    0.0       0.0  0.00  0.00   \n5  0.0    0.0    0.0    0.0    0.0        0.0    0.0       0.0  1.81  1.81   \n6  0.0    2.5    0.0    0.0    0.0        0.0    0.0       0.0  0.00  0.00   \n7  0.0    0.0    0.0    0.0    0.0        0.0    0.0       0.0  1.81  1.81   \n\n   brown  \n0   0.00  \n1   0.00  \n2   1.81  \n3   0.00  \n4   0.00  \n5   1.81  \n6   0.00  \n7   1.81   \n\nNorms:\n [3.013 3.672 4.761 6.676 5.492 4.35  5.019 4.049] \n\nFinal TF-DF feature matrix:\n     sky  blue  beautiful  love  eggs  beans  jumps  lazy  quick  saussages  \\\n0  0.60  0.53       0.60  0.00  0.00   0.00   0.00  0.00   0.00       0.00   \n1  0.49  0.43       0.49  0.57  0.00   0.00   0.00  0.00   0.00       0.00   \n2  0.00  0.00       0.00  0.00  0.00   0.00   0.53  0.38   0.38       0.00   \n3  0.00  0.00       0.00  0.00  0.31   0.38   0.00  0.00   0.00       0.00   \n4  0.00  0.00       0.00  0.38  0.38   0.00   0.00  0.00   0.00       0.46   \n5  0.00  0.37       0.00  0.00  0.00   0.00   0.00  0.42   0.42       0.00   \n6  0.72  0.32       0.36  0.00  0.00   0.00   0.00  0.00   0.00       0.00   \n7  0.00  0.00       0.00  0.00  0.00   0.00   0.00  0.45   0.45       0.00   \n\n    ham  today  green  bacon  kings  breakfast  toast  sausages   dog   fox  \\\n0  0.00    0.0   0.00   0.00   0.00       0.00   0.00      0.00  0.00  0.00   \n1  0.00    0.0   0.00   0.00   0.00       0.00   0.00      0.00  0.00  0.00   \n2  0.00    0.0   0.00   0.00   0.00       0.00   0.00      0.00  0.38  0.38   \n3  0.31    0.0   0.00   0.31   0.38       0.38   0.38      0.38  0.00  0.00   \n4  0.38    0.0   0.46   0.38   0.00       0.00   0.00      0.00  0.00  0.00   \n5  0.00    0.0   0.00   0.00   0.00       0.00   0.00      0.00  0.42  0.42   \n6  0.00    0.5   0.00   0.00   0.00       0.00   0.00      0.00  0.00  0.00   \n7  0.00    0.0   0.00   0.00   0.00       0.00   0.00      0.00  0.45  0.45   \n\n   brown  \n0   0.00  \n1   0.00  \n2   0.38  \n3   0.00  \n4   0.00  \n5   0.42  \n6   0.00  \n7   0.45   \n\nNew doc features:\n    bacon  beans  beautiful  blue  breakfast  brown  dog  eggs  fox  green  \\\n0    0.0    0.0        0.0   0.0        0.0    0.0  0.0   0.0  0.0   0.63   \n\n   ham  jumps  kings  lazy  love  quick  sausages  saussages   sky  toast  \\\n0  0.0    0.0    0.0   0.0   0.0    0.0       0.0        0.0  0.46    0.0   \n\n   today  \n0   0.63   \n\n"
     ]
    }
   ],
   "source": [
    "# compute idf diagonal matrix\n",
    "total_features = bow_features.shape[1]\n",
    "idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n",
    "idf_dense = idf_diag.todense()\n",
    "\n",
    "# print the idf diagonal matrix\n",
    "print('Diagonal matrix:\\n', pd.DataFrame(np.round(idf_dense, 2)), '\\n')\n",
    "\n",
    "# compute tfidf feature matrix - page 218\n",
    "tf = np.array(bow_features, dtype='float64')\n",
    "tfidf = tf * idf\n",
    "# view raw tfidf feature matrix\n",
    "print('Raw TF-IDF feature matrix\\n', pd.DataFrame(np.round(tfidf, 2), columns=feature_names), '\\n')\n",
    "\n",
    "# computer l2 norms\n",
    "from numpy.linalg import norm\n",
    "norms = norm(tfidf, axis=1)\n",
    "\n",
    "# print norms for each document\n",
    "print('Norms:\\n', np.round(norms, 3), '\\n')\n",
    "\n",
    "# compute normalized tfidf\n",
    "norm_tfidf = tfidf / norms[:, None]\n",
    "\n",
    "# show final tfidf feature matrix\n",
    "print('Final TF-DF feature matrix:\\n',  pd.DataFrame(np.round(norm_tfidf, 2), columns=feature_names), '\\n')\n",
    "\n",
    "# Extracting Features for New Documents - page 220\n",
    "new_doc = 'the sky is green today'\n",
    "print('New doc features:\\n', pd.DataFrame(np.round(tv.transform([new_doc]).toarray(), 2), columns=tv.get_feature_names()), '\\n')\n"
   ]
  },
  {
   "source": [
    "## Document Similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Similarity matrix DF:\n           0         1         2         3         4         5         6  \\\n0  1.000000  0.820599  0.000000  0.000000  0.000000  0.192353  0.817246   \n1  0.820599  1.000000  0.000000  0.000000  0.218401  0.157845  0.670631   \n2  0.000000  0.000000  1.000000  0.000000  0.000000  0.791821  0.000000   \n3  0.000000  0.000000  0.000000  1.000000  0.360407  0.000000  0.000000   \n4  0.000000  0.218401  0.000000  0.360407  1.000000  0.000000  0.000000   \n5  0.192353  0.157845  0.791821  0.000000  0.000000  1.000000  0.115488   \n6  0.817246  0.670631  0.000000  0.000000  0.000000  0.115488  1.000000   \n7  0.000000  0.000000  0.850516  0.000000  0.000000  0.930989  0.000000   \n\n          7  \n0  0.000000  \n1  0.000000  \n2  0.850516  \n3  0.000000  \n4  0.000000  \n5  0.930989  \n6  0.000000  \n7  1.000000   \n\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load the tv_matrix save in the last file\n",
    "tv_matrix = np.load('tv_matrix.npy')\n",
    "\n",
    "# Document Similarity - staring on page 221\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "print('Similarity matrix DF:\\n', similarity_df, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}