{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 6 - Classifying Text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "source": [
    "## Get the Data\n",
    "Unlike the book, I'm not fetching from the NLTK dataset. I have my own version, which is a little cleaner than the one on NLTK."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original shape: (18541, 3)\n                                             Article  Target Label  \\\n0  \\n\\nI am sure some bashers of Pens fans are pr...          10.0   \n1  My brother is in the market for a high-perform...           3.0   \n2  \\n\\n\\n\\n\\tFinally you said what you dream abou...          17.0   \n3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...           3.0   \n4  1)    I have an old Jasmine drive which I cann...           4.0   \n5  \\n\\nBack in high school I worked as a lab assi...          12.0   \n6  \\n\\nAE is in Dallas...try 214/241-6060 or 214/...           4.0   \n7  \\n[stuff deleted]\\n\\nOk, here's the solution t...          10.0   \n8  \\n\\n\\nYeah, it's the second one.  And I believ...          10.0   \n9  \\nIf a Christian means someone who believes in...          19.0   \n\n                Target Name  \n0          rec.sport.hockey  \n1  comp.sys.ibm.pc.hardware  \n2     talk.politics.mideast  \n3  comp.sys.ibm.pc.hardware  \n4     comp.sys.mac.hardware  \n5           sci.electronics  \n6     comp.sys.mac.hardware  \n7          rec.sport.hockey  \n8          rec.sport.hockey  \n9        talk.religion.misc  \n"
     ]
    }
   ],
   "source": [
    "data_df= pd.read_csv('data/fetch_20newsgroups.csv')\n",
    "print('Original shape:', data_df.shape)\n",
    "print(data_df.head(10))"
   ]
  },
  {
   "source": [
    "## Data Preprocessing and Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Empty documents: 130\nNew shape: (18411, 3)\n"
     ]
    }
   ],
   "source": [
    "total_nulls = data_df[data_df.Article.str.strip() == ''].shape[0]\n",
    "print('Empty documents:', total_nulls)\n",
    "\n",
    "data_df = data_df[~(data_df.Article.str.strip() == '')]\n",
    "print('New shape:', data_df.shape)"
   ]
  },
  {
   "source": [
    "## Starting on page 290 - follow my code! Author's code won't work."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting TextNormalizer\n",
      "Done strip\n",
      "Done lower\n",
      "Done stopword\n",
      "Done char remove\n",
      "Done contract exp\n",
      "Done text lemm\n",
      "Done spec char remove\n",
      "Normalizing finished in  34.18 \n",
      "\n",
      "                                             Article  \\\n",
      "0  \\n\\nI am sure some bashers of Pens fans are pr...   \n",
      "1  My brother is in the market for a high-perform...   \n",
      "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...   \n",
      "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...   \n",
      "4  1)    I have an old Jasmine drive which I cann...   \n",
      "\n",
      "                                       Clean Article  Target Label  \\\n",
      "0  sure bashers pens fans pretty confused lack ki...          10.0   \n",
      "1  brother market highperformance video card supp...           3.0   \n",
      "2  finally said dream about mediterranean new are...          17.0   \n",
      "3  think scsi card dma transfers disks scsi card ...           3.0   \n",
      "4   old jasmine drive cannot use new system under...           4.0   \n",
      "\n",
      "                Target Name  \n",
      "0          rec.sport.hockey  \n",
      "1  comp.sys.ibm.pc.hardware  \n",
      "2     talk.politics.mideast  \n",
      "3  comp.sys.ibm.pc.hardware  \n",
      "4     comp.sys.mac.hardware   \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18035 entries, 0 to 18034\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Article        18035 non-null  object \n",
      " 1   Clean Article  18035 non-null  object \n",
      " 2   Target Label   18035 non-null  float64\n",
      " 3   Target Name    18035 non-null  object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 563.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# There are several ways to get folders visible in Python. This way isn't the most elegant\n",
    "# but it works consistently. Replace my path with yours. The path you append to should be the\n",
    "# folder where your tokenizer Python class is located.\n",
    "import sys\n",
    "sys.path.append(r'YOUR PATH TO THE TEXT NORMALIZER')\n",
    "from text_normalizer import TextNormalizer # this will probably not be your path\n",
    "\n",
    "# create the normalizer object\n",
    "tn = TextNormalizer()\n",
    "\n",
    "# normalize the corpus\n",
    "import time\n",
    "start = time.time()\n",
    "norm_corpus = tn.normalize_corpus(corpus=data_df['Article'], html_stripping=True,\n",
    "                                  contraction_expansion=True, accented_char_removal=True,\n",
    "                                  text_lower_case=True, text_lemmatization=True,\n",
    "                                  special_char_removal=True, remove_digits=True,\n",
    "                                  stopword_removal=True)\n",
    "full_time = round(time.time() - start, 2)\n",
    "print('Normalizing finished in ', str(full_time), '\\n')\n",
    "\n",
    "data_df['Clean Article'] = norm_corpus\n",
    "\n",
    "# view sample data\n",
    "data_df = data_df[['Article', 'Clean Article', 'Target Label', 'Target Name']]\n",
    "print(data_df.head(), '\\n')\n",
    "\n",
    "data_df = data_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "print(data_df.info())"
   ]
  },
  {
   "source": [
    "## Save the cleaned file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('data/clean_newsgroups.csv', index=False)"
   ]
  },
  {
   "source": [
    "## Confusion Matrix - Starting on Page 292\n",
    "Building test and train data sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                Target Label  Train Count  Test Count\n0                    sci.med          652         294\n4         rec.sport.baseball          649         297\n16            comp.windows.x          643         321\n10              misc.forsale          642         300\n12          rec.sport.hockey          640         320\n14  comp.sys.ibm.pc.hardware          639         311\n1              comp.graphics          638         303\n6                  rec.autos          634         287\n19     comp.sys.mac.hardware          634         284\n13    soc.religion.christian          632         329\n7                  sci.space          631         313\n15           sci.electronics          627         315\n3            rec.motorcycles          623         332\n2      talk.politics.mideast          620         284\n8                  sci.crypt          617         320\n9    comp.os.ms-windows.misc          606         317\n17        talk.politics.guns          582         291\n11               alt.atheism          496         273\n18        talk.politics.misc          495         246\n5         talk.religion.misc          383         215 \n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load the data save to .csv from a_data.py\n",
    "# this line isn't necessary unless you start from here\n",
    "data_df = pd.read_csv('data/clean_newsgroups.csv')\n",
    "\n",
    "train_corpus, test_corpus, train_label_nums, test_label_nums, \\\n",
    "    train_label_names, test_label_names = train_test_split(np.array(data_df['Clean Article']),\n",
    "                                                          np.array(data_df['Target Label']),\n",
    "                                                          np.array(data_df['Target Name']),\n",
    "                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "from collections import Counter\n",
    "trd  = dict(Counter(train_label_names))\n",
    "tsd = dict(Counter(test_label_names))\n",
    "\n",
    "print((pd.DataFrame([[key, trd[key], tsd[key]] for key in trd],\n",
    "              columns=['Target Label', 'Train Count',\n",
    "                       'Test Count']).sort_values(by=['Train Count', 'Test Count'], ascending=False)), '\\n')"
   ]
  },
  {
   "source": [
    "## Evaluating Classification Models\n",
    "For some strange reason, the author uses a breast cancer dataset to show the model evaluation methods (versus the newsgroup dataset). \n",
    "\n",
    "### Confusion Matrix - Starting on Page 310"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion matrix: \n [[49  6]\n [ 2 86]]\n86 6 49 2 \n\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.75,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=1234)\n",
    "\n",
    "# train and build the model\n",
    "logistic = linear_model.LogisticRegression(solver='lbfgs', max_iter=5000)\n",
    "logistic.fit(X_train, y_train)\n",
    "# predict on test data and view confusion matrix\n",
    "y_pred = logistic.predict(X_test)\n",
    "\n",
    "# note this is a standard package, not the one in the book on page 310\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "print('Confusion matrix: \\n', confusion_matrix)\n",
    "\n",
    "# Performance Metrics, starting on page 312\n",
    "positive_class = 1\n",
    "TP = confusion_matrix[1, 1]\n",
    "FP = confusion_matrix[0, 1]\n",
    "TN = confusion_matrix[0, 0]\n",
    "FN = confusion_matrix[1, 0]\n",
    "print(TP, FP, TN, FN, '\\n')"
   ]
  },
  {
   "source": [
    "### Accuracy, Precision, and Recall"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Framework Accuracy: 0.94406\nManually Computed Accuracy: 0.94406 \n\nFramework Precision: 0.93478\nManually Computed Precision: 0.93478 \n\nFramework Recall: 0.97727\nManually computed Recall: 0.97727 \n\nFramework F1-Score: 0.95556\nManually Computed F1-Score: 0.95555\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score # standard\n",
    "print('Framework Accuracy:', round(accuracy_score(y_test, y_pred), 5))\n",
    "mc_acc = round((TP + TN) / (TP + TN + FP + FN), 5)\n",
    "print('Manually Computed Accuracy:', mc_acc, '\\n')\n",
    "\n",
    "# precision\n",
    "from sklearn.metrics import precision_score\n",
    "print('Framework Precision:', round(precision_score(y_test, y_pred), 5))\n",
    "mc_prec = round((TP) / (TP + FP), 5)\n",
    "print('Manually Computed Precision:', mc_prec, '\\n')\n",
    "\n",
    "# recall\n",
    "from sklearn.metrics import recall_score\n",
    "print('Framework Recall:', round(recall_score(y_test, y_pred), 5))\n",
    "mc_rec = round((TP) / (TP + FN), 5)\n",
    "print('Manually computed Recall:', mc_rec, '\\n')\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print('Framework F1-Score:', round(f1_score(y_test, y_pred), 5))\n",
    "mc_f1 = round((2*mc_prec*mc_rec) / (mc_prec+mc_rec), 5)\n",
    "print('Manually Computed F1-Score:', mc_f1)"
   ]
  },
  {
   "source": [
    "## BOW Features with Classification Models - starting on page 315\n",
    "We go back to the newsgroup dataset - you don't need to reload it, but I do so for completeness."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "clean_df = pd.read_csv('data/clean_newsgroups.csv')\n",
    "\n",
    "# cleaning the articles created some NaNs - get rid of those\n",
    "data_df = clean_df[clean_df['Clean Article'].notna()]\n",
    "\n",
    "# split to test and train\n",
    "train_corpus, test_corpus, train_label_nums, test_label_nums, \\\n",
    "    train_label_names, test_label_names = train_test_split(np.array(data_df['Clean Article']),\n",
    "                                                          np.array(data_df['Target Label']),\n",
    "                                                          np.array(data_df['Target Name']),\n",
    "                                                          test_size=0.33, random_state=42)"
   ]
  },
  {
   "source": [
    "### Build BOW features on train articles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train features shape: (12072, 67312)\nTest features shape: (5946, 67312) \n\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0)\n",
    "cv_train_features = cv.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "cv_test_features = cv.transform(test_corpus)\n",
    "\n",
    "print('Train features shape:', cv_train_features.shape)\n",
    "print('Test features shape:', cv_test_features.shape, '\\n')"
   ]
  },
  {
   "source": [
    "## Other Models - Starting on Page 316\n",
    "This takes a while to run."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Naive Bayes:\n",
      "CV Accuracy (5-fold): [0.71546392 0.69549028 0.69014085 0.70705394 0.69534497]\n",
      "Mean CV Accuracy: 0.7006987905228245\n",
      "Test Accuracy: 0.7233434241506895 \n",
      "\n",
      "Logistic Regression:\n",
      "CV Accuracy (5-fold): [0.68123711 0.65742656 0.65824358 0.65975104 0.67414796]\n",
      "Mean CV Accuracy: 0.6661612510293118\n",
      "Test Accuracy: 0.6907164480322906 \n",
      "\n",
      "SVM:\n",
      "CV Accuracy (5-fold): [0.63793814 0.62019032 0.62551781 0.62448133 0.63923525]\n",
      "Mean CV Accuracy: 0.6294725697373328\n",
      "Test Accuracy: 0.6560713084426505 \n",
      "\n",
      "Random Forest\n",
      "CV Accuracy (5-fold): [0.53030928 0.53868432 0.52278376 0.52946058 0.56109726]\n",
      "Mean CV Accuracy: 0.536467039383467\n",
      "Test Accuracy: 0.5613858055835856 \n",
      "\n",
      "Gradient Boosting\n",
      "CV Accuracy (5-fold): [0.52783505 0.51634257 0.52236951 0.55352697 0.54862843]\n",
      "Mean CV Accuracy: 0.5337405072102663\n",
      "Test Accuracy: 0.5455768583921964\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes:')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(cv_train_features, train_label_names)\n",
    "mnb_bow_cv_scores = cross_val_score(mnb, cv_train_features, train_label_names, cv=5)\n",
    "mnb_bow_cv_mean_score = np.mean(mnb_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', mnb_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', mnb_bow_cv_mean_score)\n",
    "mnb_bow_test_score = mnb.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', mnb_bow_test_score, '\\n')\n",
    "\n",
    "# Logistic Regression - starting on page 316\n",
    "# This takes quite a while to run, be patient.\n",
    "print('Logistic Regression:')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=1000, solver='lbfgs',\n",
    "                        C=1, random_state=42, multi_class='auto')\n",
    "lr.fit(cv_train_features, train_label_names)\n",
    "lr_bow_cv_scores = cross_val_score(lr, cv_train_features, train_label_names, cv=5)\n",
    "lr_bow_cv_mean_score = np.mean(lr_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', lr_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', lr_bow_cv_mean_score)\n",
    "lr_bow_test_score = lr.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', lr_bow_test_score, '\\n')\n",
    "\n",
    "# Support Vector Machines\n",
    "print('SVM:')\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(penalty='l2', max_iter=1000, C=1, random_state=42)\n",
    "svm.fit(cv_train_features, train_label_names)\n",
    "svm_bow_cv_scores = cross_val_score(svm, cv_train_features, train_label_names, cv=5)\n",
    "svm_bow_cv_mean_score = np.mean(svm_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_bow_cv_mean_score)\n",
    "svm_bow_test_score = svm.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_bow_test_score, '\\n')\n",
    "\n",
    "# Random Forest\n",
    "print('Random Forest')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rfc.fit(cv_train_features, train_label_names)\n",
    "rfc_bow_cv_scores = cross_val_score(rfc, cv_train_features, train_label_names, cv=5)\n",
    "rfc_bow_cv_mean_score = np.mean(rfc_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', rfc_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', rfc_bow_cv_mean_score)\n",
    "rfc_bow_test_score = rfc.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', rfc_bow_test_score, '\\n')\n",
    "\n",
    "# Gradient Boosting Machines\n",
    "# This takes quite a while to run...\n",
    "print('Gradient Boosting')\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=10, random_state=42)\n",
    "gbc.fit(cv_train_features, train_label_names)\n",
    "gbc_bow_cv_scores = cross_val_score(gbc, cv_train_features, train_label_names, cv=5)\n",
    "gbc_bow_cv_mean_score = np.mean(gbc_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', gbc_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', gbc_bow_cv_mean_score)\n",
    "gbc_bow_test_score = gbc.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', gbc_bow_test_score)"
   ]
  },
  {
   "source": [
    "## TF-IDF Features with Classification Models - Starting on page 319"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train features shape: (12072, 67312)\n Test features shape: (5946, 67312) \n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# build BOW features on train articles\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0)\n",
    "tv_train_features = tv.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "tv_test_features = tv.transform(test_corpus)\n",
    "\n",
    "print('Train features shape:', tv_train_features.shape)\n",
    "print(' Test features shape:', tv_test_features.shape, '\\n')"
   ]
  },
  {
   "source": [
    "## Building Models with TF-IDF Features\n",
    "This takes a while to run..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Naive Bayes:\n",
      "CV Accuracy (5-fold): [0.70721649 0.69962764 0.68889809 0.70788382 0.6957606 ]\n",
      "Mean CV Accuracy: 0.6998773285585533\n",
      "Test Accuracy: 0.7119071644803229 \n",
      "\n",
      "Logistic Regression:\n",
      "CV Accuracy (5-fold): [0.74762887 0.72776169 0.72783761 0.74273859 0.72942643]\n",
      "Mean CV Accuracy: 0.7350786382136094\n",
      "Test Accuracy: 0.7514295324587958 \n",
      "\n",
      "Support Vector Machines:\n",
      "CV Accuracy (5-fold): [0.75051546 0.74141498 0.73695112 0.75809129 0.74729842]\n",
      "Mean CV Accuracy: 0.7468542533119572\n",
      "Test Accuracy: 0.767238479650185 \n",
      "\n",
      "SVM With Gradient Descent:\n",
      "CV Accuracy (5-fold): [0.76123711 0.74886223 0.74233637 0.76348548 0.74480466]\n",
      "Mean CV Accuracy: 0.752145168535528\n",
      "Test Accuracy: 0.7684157416750756 \n",
      "\n",
      "Random Forest:\n",
      "CV Accuracy (5-fold): [0.54927835 0.51592884 0.54598177 0.54149378 0.5361596 ]\n",
      "Mean CV Accuracy: 0.5377684675678408\n",
      "Test Accuracy: 0.5593676421123445 \n",
      "\n",
      "Grandient Boosting:\n",
      "CV Accurance (5-fold): [0.53690722 0.51592884 0.52568351 0.55186722 0.54904406]\n",
      "Mean CV Accuracy: 0.535886168636141\n",
      "Test Accuracy: 0.5457450386814665 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "print('Naive Bayes:')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(tv_train_features, train_label_names)\n",
    "mnb_tfidf_cv_scores = cross_val_score(mnb, tv_train_features, train_label_names, cv=5)\n",
    "mnb_tfidf_cv_mean_score = np.mean(mnb_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', mnb_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', mnb_tfidf_cv_mean_score)\n",
    "mnb_tfidf_test_score = mnb.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', mnb_tfidf_test_score, '\\n')\n",
    "\n",
    "# Logistic Regression\n",
    "print('Logistic Regression:')\n",
    "# This takes quite a while to run, be patient.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, solver='lbfgs',\n",
    "                        C=1, random_state=42, multi_class='auto')\n",
    "lr.fit(tv_train_features, train_label_names)\n",
    "lr_tfidf_cv_scores = cross_val_score(lr, tv_train_features, train_label_names, cv=5)\n",
    "lr_tfidf_cv_mean_score = np.mean(lr_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', lr_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', lr_tfidf_cv_mean_score)\n",
    "lr_tfidf_test_score = lr.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', lr_tfidf_test_score, '\\n')\n",
    "\n",
    "# Support Vector Machines\n",
    "print('Support Vector Machines:')\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(tv_train_features, train_label_names)\n",
    "svm_tfidf_cv_scores = cross_val_score(svm, tv_train_features, train_label_names, cv=5)\n",
    "svm_tfidf_cv_mean_score = np.mean(svm_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_tfidf_cv_mean_score)\n",
    "svm_tfidf_test_score = svm.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_tfidf_test_score, '\\n')\n",
    "\n",
    "# SVM with Stochastic Gradient Descent\n",
    "print('SVM With Gradient Descent:')\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_sgd = SGDClassifier(loss='hinge', penalty='l2', max_iter=50, tol=1e-3)\n",
    "svm_sgd.fit(tv_train_features, train_label_names)\n",
    "svmsgd_tfidf_cv_scores = cross_val_score(svm_sgd, tv_train_features, train_label_names, cv=5)\n",
    "svmsg_tfidf_cv_mean_score = np.mean(svmsgd_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svmsgd_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', svmsg_tfidf_cv_mean_score)\n",
    "svmsgd_tfidf_test_score = svm_sgd.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svmsgd_tfidf_test_score, '\\n')\n",
    "\n",
    "# Random Forest\n",
    "print('Random Forest:')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=42) \n",
    "rfc.fit(tv_train_features, train_label_names)\n",
    "rfc_tfidf_cv_scores = cross_val_score(rfc, tv_train_features, train_label_names, cv=5)\n",
    "rfc_tfidf_cv_mean_score = np.mean(rfc_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', rfc_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', rfc_tfidf_cv_mean_score)\n",
    "rfc_tfidf_test_score = rfc.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', rfc_tfidf_test_score, '\\n')\n",
    "\n",
    "# Gradient Boosting\n",
    "print('Grandient Boosting:')\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=10, random_state=42)\n",
    "gbc.fit(tv_train_features, train_label_names)\n",
    "gbc_tfidf_cv_scores = cross_val_score(gbc, tv_train_features, train_label_names, cv=5)\n",
    "gbc_tfidf_cv_mean_score = np.mean(gbc_tfidf_cv_scores)\n",
    "print('CV Accurance (5-fold):', gbc_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', gbc_tfidf_cv_mean_score)\n",
    "gbc_tfidf_test_score = gbc.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', gbc_tfidf_test_score, '\\n')"
   ]
  },
  {
   "source": [
    "### Comparative Model Performance - starting on page 322\n",
    "This also take a while to run."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Model  CV Score (TF-IDF)  Test Score (TF-IDF)\n0    NB           0.699877             0.711907\n1    LR           0.735079             0.751430\n2   SVM           0.746854             0.767238\n3   SGD           0.752145             0.768416\n4    RF           0.537768             0.559368\n5   GBM           0.535886             0.545745\n"
     ]
    }
   ],
   "source": [
    "# Uses only TF-IDF\n",
    "print(pd.DataFrame(\n",
    "    [['NB', mnb_tfidf_cv_mean_score, mnb_tfidf_test_score],\n",
    "    ['LR', lr_tfidf_cv_mean_score, lr_tfidf_test_score],\n",
    "    ['SVM', svm_tfidf_cv_mean_score, svm_tfidf_test_score],\n",
    "    ['SGD', svmsg_tfidf_cv_mean_score, svmsgd_tfidf_test_score],\n",
    "    ['RF', rfc_tfidf_cv_mean_score, rfc_tfidf_test_score],\n",
    "    ['GBM', gbc_tfidf_cv_mean_score, gbc_tfidf_test_score]],\n",
    "    columns=['Model', 'CV Score (TF-IDF)', 'Test Score (TF-IDF)']))"
   ]
  },
  {
   "source": [
    "## Embeddings - Starting on page 323\n",
    "\n",
    "This next block of code is not part of the book, but avoids loading the entire Tokenizer class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not in the book but is part of the author's normalization.py file\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "source": [
    "### Word2Vec Embeddings with Classification Models\n",
    "This takes a while to run."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "\n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype='float64')\n",
    "        num_words = 0. \n",
    "\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                num_words = num_words + 1. \n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        if num_words:\n",
    "            feature_vector = np.divide(feature_vector, num_words)\n",
    "        \n",
    "        return feature_vector\n",
    "    \n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "# tokenize corpus\n",
    "tokenized_train = [tokenize_text(text) for text in train_corpus]\n",
    "tokenized_test = [tokenize_text(text) for text in test_corpus]\n",
    "\n",
    "# generate word2vec word embeddings\n",
    "import gensim\n",
    "# build word2vec model\n",
    "w2v_num_features = 1000\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=100, \n",
    "                                   min_count=2, sample=1e-3, sg=1, iter=5, workers=10)"
   ]
  },
  {
   "source": [
    "### Generate document level embeddings\n",
    "Remember we only use train dataset vocabulary embeddings so that test dateset truly remains an unseen dataset\n",
    "\n",
    "#### Generate averaged word vector features from word2vec model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2VTrain features shape: (12072, 1000)\nTest features shape: (5946, 1000) \n\n"
     ]
    }
   ],
   "source": [
    "avg_wv_train_features = document_vectorizer(corpus=tokenized_train, model=w2v_model, \n",
    "                                            num_features=w2v_num_features)\n",
    "avg_wv_test_features = document_vectorizer(corpus=tokenized_test, model=w2v_model, \n",
    "                                           num_features=w2v_num_features)\n",
    "\n",
    "print('Word2VTrain features shape:', avg_wv_train_features.shape)\n",
    "print('Test features shape:', avg_wv_test_features.shape, '\\n')"
   ]
  },
  {
   "source": [
    "## Create SGDClassiferi Model with Word2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CV Accuracy (5-fold): [0.72907216 0.73438146 0.72576636 0.72365145 0.72693267]\nMean CV Accuracy: 0.7279608226137078\nTest Accuracy: 0.7221661621257989 \n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', max_iter=50, tol=1e-3)\n",
    "svm.fit(avg_wv_train_features, train_label_names)\n",
    "svm_w2v_cv_scores = cross_val_score(svm, avg_wv_train_features, train_label_names, cv=5)\n",
    "svm_w2v_cv_mean_score = np.mean(svm_w2v_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_w2v_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_w2v_cv_mean_score)\n",
    "svm_w2v_test_score = svm.score(avg_wv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_w2v_test_score, '\\n')"
   ]
  },
  {
   "source": [
    "Skipping GloVe and FastText - same ideas as W2V and take a while to run\n",
    "\n",
    "Skipping the neural network - also computationally expensive and hasn't been introduced"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Model Tuning and Performance - Starting on Page 329\n",
    "### Tuning our Multinomial Naive Bayes Model - this take a while to run (but not too long)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 1) .....................\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 1) .....................\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 1) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 1) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 1) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 2) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 2), total=   5.0s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 2) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 2), total=   5.0s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 2) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 2), total=   4.8s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 2) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 2), total=   5.0s\n",
      "[CV] mnb__alpha=1e-05, tfidf__ngram_range=(1, 2) .....................\n",
      "[CV] ...... mnb__alpha=1e-05, tfidf__ngram_range=(1, 2), total=   5.3s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 1), total=   0.8s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 1), total=   1.0s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 1) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 2), total=   4.9s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 2), total=   4.6s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 2), total=   4.7s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 2), total=   5.1s\n",
      "[CV] mnb__alpha=0.0001, tfidf__ngram_range=(1, 2) ....................\n",
      "[CV] ..... mnb__alpha=0.0001, tfidf__ngram_range=(1, 2), total=   5.2s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 1) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 1) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 1) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 1) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 1) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 1), total=   1.0s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 2) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 2), total=   5.0s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 2) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 2), total=   5.1s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 2) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 2), total=   5.6s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 2) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 2), total=   6.2s\n",
      "[CV] mnb__alpha=0.01, tfidf__ngram_range=(1, 2) ......................\n",
      "[CV] ....... mnb__alpha=0.01, tfidf__ngram_range=(1, 2), total=   6.2s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 1) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 1), total=   1.4s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 1) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 1), total=   1.0s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 1) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 1), total=   1.0s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 1) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 1), total=   1.0s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 1) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 1), total=   1.0s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 2) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 2), total=   4.8s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 2) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 2), total=   4.5s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 2) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 2), total=   4.8s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 2) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 2), total=   4.6s\n",
      "[CV] mnb__alpha=0.1, tfidf__ngram_range=(1, 2) .......................\n",
      "[CV] ........ mnb__alpha=0.1, tfidf__ngram_range=(1, 2), total=   4.8s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 1) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 1), total=   0.8s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 1) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 1) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 1), total=   0.8s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 1) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 1), total=   0.8s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 1) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 1), total=   1.0s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 2) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 2), total=   5.6s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 2) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 2), total=   4.8s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 2) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 2), total=   5.2s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 2) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 2), total=   5.1s\n",
      "[CV] mnb__alpha=1, tfidf__ngram_range=(1, 2) .........................\n",
      "[CV] .......... mnb__alpha=1, tfidf__ngram_range=(1, 2), total=   5.0s\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  3.7min finished\n",
      "{'memory': None, 'steps': [('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('mnb', MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))], 'tfidf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'mnb': MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True), 'tfidf__analyzer': 'word', 'tfidf__binary': False, 'tfidf__decode_error': 'strict', 'tfidf__dtype': <class 'numpy.float64'>, 'tfidf__encoding': 'utf-8', 'tfidf__input': 'content', 'tfidf__lowercase': True, 'tfidf__max_df': 1.0, 'tfidf__max_features': None, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 2), 'tfidf__norm': 'l2', 'tfidf__preprocessor': None, 'tfidf__smooth_idf': True, 'tfidf__stop_words': None, 'tfidf__strip_accents': None, 'tfidf__sublinear_tf': False, 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__tokenizer': None, 'tfidf__use_idf': True, 'tfidf__vocabulary': None, 'mnb__alpha': 0.01, 'mnb__class_prior': None, 'mnb__fit_prior': True} \n",
      "\n",
      "Modeling tuning results DF:    rank                                                params  \\\n",
      "5     1    {'mnb__alpha': 0.01, 'tfidf__ngram_range': (1, 2)}   \n",
      "4     2    {'mnb__alpha': 0.01, 'tfidf__ngram_range': (1, 1)}   \n",
      "6     3     {'mnb__alpha': 0.1, 'tfidf__ngram_range': (1, 1)}   \n",
      "3     4  {'mnb__alpha': 0.0001, 'tfidf__ngram_range': (1, 2)}   \n",
      "7     5     {'mnb__alpha': 0.1, 'tfidf__ngram_range': (1, 2)}   \n",
      "1     6   {'mnb__alpha': 1e-05, 'tfidf__ngram_range': (1, 2)}   \n",
      "2     7  {'mnb__alpha': 0.0001, 'tfidf__ngram_range': (1, 1)}   \n",
      "0     8   {'mnb__alpha': 1e-05, 'tfidf__ngram_range': (1, 1)}   \n",
      "8     9       {'mnb__alpha': 1, 'tfidf__ngram_range': (1, 1)}   \n",
      "9    10       {'mnb__alpha': 1, 'tfidf__ngram_range': (1, 2)}   \n",
      "\n",
      "   cv score (mean)  cv score (std)  \n",
      "5         0.763171        0.007106  \n",
      "4         0.760769        0.002593  \n",
      "6         0.749089        0.005084  \n",
      "3         0.746852        0.003499  \n",
      "7         0.745444        0.007998  \n",
      "1         0.738403        0.005915  \n",
      "2         0.732025        0.002351  \n",
      "0         0.720345        0.002957  \n",
      "8         0.701706        0.006218  \n",
      "9         0.698393        0.007478   \n",
      "\n",
      "Test Accuracy: 0.7791792801883619 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "mnb_pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('mnb', MultinomialNB())])\n",
    "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)], 'mnb__alpha': [1e-5, 1e-4, 1e-2, 1e-1, 1]}\n",
    "\n",
    "gs_mnb = GridSearchCV(mnb_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_mnb = gs_mnb.fit(train_corpus, train_label_names)\n",
    "\n",
    "print(gs_mnb.best_estimator_.get_params(), '\\n')\n",
    "\n",
    "cv_results = gs_mnb.cv_results_\n",
    "results_df = pd.DataFrame({'rank': cv_results['rank_test_score'],\n",
    "                            'params': cv_results['params'], \n",
    "                            'cv score (mean)': cv_results['mean_test_score'],\n",
    "                            'cv score (std)': cv_results['std_test_score']})\n",
    "results_df = results_df.sort_values(by=['rank'], ascending=True)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "print('Modeling tuning results DF:', results_df, '\\n')\n",
    "\n",
    "best_mnb_test_score = gs_mnb.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy:', best_mnb_test_score, '\\n')"
   ]
  },
  {
   "source": [
    "## Tuning our Logistic Regression Model\n",
    "Also takes a while to run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 1) ..............................\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 1), total=   4.2s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 1) ..............................\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.8s remaining:    0.0s\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 1), total=   4.1s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 1), total=   4.3s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 1), total=   4.2s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 1), total=   4.1s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 2), total=  18.4s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 2), total=  16.2s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 2), total=  15.1s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 2), total=  16.5s\n",
      "[CV] lr__C=1, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=1, tfidf__ngram_range=(1, 2), total=  17.9s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 1), total=   5.2s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 1), total=   5.1s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 1), total=   4.5s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 1), total=   5.1s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 1) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 2), total=  19.9s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 2), total=  22.4s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 2), total=  20.3s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 2), total=  20.2s\n",
      "[CV] lr__C=5, tfidf__ngram_range=(1, 2) ..............................\n",
      "[CV] ............... lr__C=5, tfidf__ngram_range=(1, 2), total=  18.6s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 1), total=   5.1s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 1), total=   5.7s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 1), total=   5.5s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 1), total=   5.3s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 1), total=   5.2s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 2), total=  21.2s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 2), total=  21.0s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 2), total=  21.1s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 2), total=  22.0s\n",
      "[CV] lr__C=10, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. lr__C=10, tfidf__ngram_range=(1, 2), total=  23.1s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  6.8min finished\n",
      "Test Accuracy: 0.767238479650185 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('lr', LogisticRegression(penalty='l2', \n",
    "                          max_iter=100, random_state=42))])\n",
    "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)], 'lr__C': [1, 5, 10]}\n",
    "\n",
    "gl_lr = GridSearchCV(lr_pipeline, param_grid, cv=5, verbose=2)\n",
    "gl_lr = gl_lr.fit(train_corpus, train_label_names)\n",
    "\n",
    "# evaluate best tuned model on the test dataset\n",
    "best_lr_test_score = gl_lr.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy:', best_lr_test_score, '\\n')"
   ]
  },
  {
   "source": [
    "## Tuning the Linear SVM Model\n",
    "Also takes a while to run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.1s remaining:    0.0s\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.4s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 1) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   6.4s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   6.2s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   7.6s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   6.8s\n",
      "[CV] svm__C=0.01, tfidf__ngram_range=(1, 2) ..........................\n",
      "[CV] ........... svm__C=0.01, tfidf__ngram_range=(1, 2), total=   5.8s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   1.4s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   1.5s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 1) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 1), total=   1.4s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   6.7s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   7.2s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   6.3s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   6.3s\n",
      "[CV] svm__C=0.1, tfidf__ngram_range=(1, 2) ...........................\n",
      "[CV] ............ svm__C=0.1, tfidf__ngram_range=(1, 2), total=   6.4s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 1), total=   1.6s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 1), total=   1.6s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 1), total=   1.6s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 1), total=   1.6s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 1), total=   1.6s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 2), total=   8.4s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 2), total=   8.5s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 2), total=   8.4s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 2), total=   8.7s\n",
      "[CV] svm__C=1, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=1, tfidf__ngram_range=(1, 2), total=   8.5s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 1), total=   2.7s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 1), total=   2.8s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 1), total=   2.8s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 1) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 1), total=   2.8s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 2), total=  22.2s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 2), total=  19.3s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 2), total=  17.2s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 2), total=  18.6s\n",
      "[CV] svm__C=5, tfidf__ngram_range=(1, 2) .............................\n",
      "[CV] .............. svm__C=5, tfidf__ngram_range=(1, 2), total=  17.4s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  4.9min finished\n",
      "Test Accuracy: 0.7780020181634713\n",
      "Unique classes: ['comp.sys.mac.hardware', 'talk.politics.guns', 'sci.med', 'talk.politics.misc', 'comp.os.ms-windows.misc', 'rec.sport.baseball', 'sci.electronics', 'rec.sport.hockey', 'sci.space', 'alt.atheism', 'comp.sys.ibm.pc.hardware', 'talk.religion.misc', 'talk.politics.mideast', 'comp.graphics', 'soc.religion.christian', 'misc.forsale', 'comp.windows.x', 'rec.motorcycles', 'sci.crypt', 'rec.autos'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm_pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('svm', LinearSVC(random_state=42))])\n",
    "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)], 'svm__C': [0.01, 0.1, 1, 5]}\n",
    "\n",
    "gs_svm = GridSearchCV(svm_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_svm = gs_svm.fit(train_corpus, train_label_names)\n",
    "\n",
    "# evaluating best tuned model on the data set\n",
    "best_svm_test_score = gs_svm.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy:', best_svm_test_score)\n",
    "\n",
    "mnb_predictions = gs_mnb.predict(test_corpus)\n",
    "unique_classes = list(set(test_label_names))\n",
    "print('Unique classes:', unique_classes, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}